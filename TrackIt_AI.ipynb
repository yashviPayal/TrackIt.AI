{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb87198-fc8e-4dcf-a979-f7e8d53816f5",
   "metadata": {},
   "source": [
    "# TrackIt.AI: Autonomous Habit Tracker\n",
    "\n",
    "**Project Goal:** To build an intelligent, state-aware agent that translates messy natural language input into reliable, automated habit tracking and personalized coaching.\n",
    "\n",
    "### Core Concepts Demonstrated:\n",
    "\n",
    "This project showcases the application of the following mandatory course concepts:\n",
    "\n",
    "1.  **LLM-Powered Agent:** Uses Groq for high-speed, deterministic intent classification and personalized response generation.\n",
    "2.  **Custom Tools:** Implements `update_streak_and_history` and `get_adaptive_context` to execute secure business logic and manage data persistence.\n",
    "3.  **Sessions & State Management:** Utilizes the global `HABIT_STORE` and `USER_PROFILE` as an in-memory session service to track streaks and provide context-aware feedback.\n",
    "\n",
    "**Instructions:** Run all cells sequentially. Add your API key.Enter habit updates (e.g., \"I walked 10k steps today\") in the text box below the dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d03542-081e-4f14-a165-cf94647f8642",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22b187-24ce-433f-8180-9066a6652bb9",
   "metadata": {},
   "source": [
    "# Why Did I Choose This Project? (My Personal Motivation)\n",
    "\n",
    "I chose this specific project because I‚Äôve personally experienced the **Habit Friction Gap**. We all know how important habits are, but the tools we use‚Äîthe standard apps‚Äîare often part of the problem. They feel like homework. When I was feeling low on motivation, opening the app, finding the log button, and manually entering '10 minutes of reading' felt like too much effort, so I‚Äôd just quit.\n",
    "\n",
    "The moment I realized the agent could be the solution was when I saw it could **eliminate that friction**. Instead of making the human change their behavior to fit the app's structure, the AI should **adapt to the human**.\n",
    "\n",
    "This project is exciting because it pushes the boundary beyond simple tracking. It lets us use the speed of Groq and the intelligence of an **LLM-Powered Agent** to create a tool that is genuinely **empathetic, personalized, and invisible**‚Äîmaking sustainable personal growth effortless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325d2cc-ccd4-4fd3-9e9d-5faea261d109",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd58780-1450-4b7a-a062-4ec2e49328bb",
   "metadata": {},
   "source": [
    "## Architecture Overview: How TrackIt.AI Works\n",
    "\n",
    "Think of the TrackIt.AI Agent as a super-fast, specialized assistant with its own secure notebook.\n",
    "\n",
    "1.  **You Speak (Input):** You send a messy text message, like: \"I only read for 5 minutes.\" This is the friction-free part. \n",
    "2.  **The Brain Translates (LLM Agent):** Our AI brain (powered by Groq for speed) instantly translates that message into a simple, structured command: *`Action: Log Success, Habit: Reading, Value: 5`*.\n",
    "3.  **The Hands Act (Custom Tool):** The agent then uses its \"hands\" (a secure Python **Custom Tool**) to open its notebook (**Session State**) and update your records. It checks your old streak, makes it a new streak (e.g., 4 days!), and closes the book.\n",
    "4.  **The Coach Responds (Output):** The agent uses the *new* streak data to give you a personalized, non-judgmental high-five: \"Awesome job on the 4-day streak! Even 5 minutes is a win.\"\n",
    "\n",
    "**In short: The AI does all the boring, structured work behind the scenes so you only have to focus on the conversation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f097b6-5d31-4872-a86c-a97757a93ff0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac543035-33c4-4156-9809-5fb53d8fd0ea",
   "metadata": {},
   "source": [
    "# Entire Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "207c6f38-5aa4-4720-a856-f04537fe5bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://a2707775be50189ea2.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a2707775be50189ea2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<- LLM Classification: {'intent': 'update_habit', 'habit': 'read', 'duration': 2, 'book': 'The Atomic Habits'}\n",
      "<- LLM Classification: {'intent': 'update_habit', 'habit': 'walk', 'data': {'steps': 8000}}\n",
      "<- LLM Classification: {'intent': 'request_small_action', 'habit': 'meditate'}\n",
      "<- LLM Classification: {'intent': 'no_action'}\n",
      "<- LLM Classification: {'intent': 'request_small_action', 'data': {}}\n",
      "<- LLM Classification: {'intent': 'unknown', 'data': {}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "import gradio as gr\n",
    "from groq import Groq \n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# =========================================================================\n",
    "# 1. SETUP & GLOBAL STATE (CONCEPT: SESSIONS & STATE MANAGEMENT)\n",
    "# =========================================================================\n",
    "\n",
    "# NOTE: Replace with your actual Groq API Key\n",
    "# os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY_HERE\"\n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\" \n",
    "\n",
    "# Global state represents the persistent user session data (In-MemorySessionService)\n",
    "HABIT_STORE: Dict[str, Any] = {\n",
    "    \"walk\": {\"goal\": 10000, \"unit\": \"steps\", \"streak\": 0, \"history\": [], \"last_logged\": None, \"min_version\": 500},\n",
    "    \"meditate\": {\"goal\": 10, \"unit\": \"minutes\", \"streak\": 0, \"history\": [], \"last_logged\": None, \"min_version\": 2},\n",
    "    \"read\": {\"goal\": 30, \"unit\": \"minutes\", \"streak\": 0, \"history\": [], \"last_logged\": None, \"min_version\": 1},\n",
    "}\n",
    "USER_PROFILE = {\"last_failure\": \"meditate\", \"last_failure_date\": \"Yesterday\"}\n",
    "\n",
    "# --- Groq LLM Client ---\n",
    "class GroqLLMClient:\n",
    "    def __init__(self, model: str):\n",
    "        # NOTE: Assumes Groq API key is in environment variables or passed directly\n",
    "        # You may need to initialize your client if not using an environment variable\n",
    "        try:\n",
    "             self.client = Groq()\n",
    "        except Exception:\n",
    "             # Fallback for demonstration if key isn't set, replace with actual initialization\n",
    "             print(\"Warning: Groq client failed to initialize. Ensure GROQ_API_KEY is set.\")\n",
    "             self.client = None \n",
    "\n",
    "        self.model = model\n",
    "        \n",
    "        # System prompt defines the LLM Agent's role and structured output requirements\n",
    "        self.system_prompt = (\n",
    "            \"You are an AI habit agent. Your task is to act as a highly precise JSON parser. \"\n",
    "            \"Based ONLY on the user's input, classify the intent and extract the required data. \"\n",
    "            \"The output MUST be a single JSON object. Do not add any extra text or conversation. \"\n",
    "            \"Possible habits: walk (steps), meditate (minutes), read (minutes). \"\n",
    "            \"Possible intents: update_habit, request_small_action, no_action, unknown.\"\n",
    "            \"If the user asks to log a habit, use 'update_habit'. If the user expresses fatigue or laziness, use 'request_small_action'.\"\n",
    "        )\n",
    "\n",
    "    def get_structured_response(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"LLM Agent's Classification Task.\"\"\"\n",
    "        if not self.client: return {\"intent\": \"unknown\", \"error\": \"Client not initialized.\"}\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"User says: '{user_input}'\"},\n",
    "                ],\n",
    "                model=self.model,\n",
    "                response_format={\"type\": \"json_object\"}, \n",
    "                temperature=0.0\n",
    "            )\n",
    "            json_str = chat_completion.choices[0].message.content.strip()\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Error fallback: demonstrates robust error handling\n",
    "            print(f\"Groq Classification Error: {e}\")\n",
    "            return {\"intent\": \"unknown\", \"error\": str(e)}\n",
    "\n",
    "    def generate_response(self, classification_result: Dict[str, Any], context: str) -> str:\n",
    "        \"\"\"LLM Agent's Natural Language Generation (NLG) Task.\"\"\"\n",
    "        if not self.client: return \"AI Response Error: Client not initialized.\"\n",
    "        intent = classification_result.get(\"intent\", \"unknown\")\n",
    "        habit = classification_result.get(\"habit\")\n",
    "        \n",
    "        nlg_prompt = \"\"\n",
    "        if intent == \"update_habit\":\n",
    "            streak = HABIT_STORE.get(habit, {}).get('streak', 0)\n",
    "            nlg_prompt = (\n",
    "                f\"The user successfully logged progress for {habit}. Current streak: {streak}. \"\n",
    "                f\"Context: {context}. Generate a short, highly motivating, and personalized response.\"\n",
    "            )\n",
    "        elif intent == \"request_small_action\" and habit:\n",
    "            min_val = HABIT_STORE.get(habit, {}).get(\"min_version\", 1)\n",
    "            unit = HABIT_STORE.get(habit, {}).get(\"unit\", \"unit\")\n",
    "            nlg_prompt = (\n",
    "                f\"The user feels lazy about '{habit}'. Remind them of the '2-Minute Rule': \"\n",
    "                f\"Ask them to do the bare minimum: '{min_val} {unit}' of {habit}. Generate a guilt-free response.\"\n",
    "            )\n",
    "        else:\n",
    "            nlg_prompt = \"The user input was confusing or logged nothing. Generate a polite message.\"\n",
    "        \n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": nlg_prompt}],\n",
    "                model=self.model,\n",
    "                temperature=0.7 \n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            return f\"AI Response Error: Failed to generate response ({e}).\"\n",
    "\n",
    "llm_client = GroqLLMClient(model=GROQ_MODEL)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 2. CUSTOM TOOLS & BUSINESS LOGIC (CONCEPT: CUSTOM TOOLS)\n",
    "# =========================================================================\n",
    "\n",
    "def update_streak_and_history(habit_name: str, value: float):\n",
    "    \"\"\"CUSTOM TOOL: Updates the HABIT_STORE (The 'database' action).\"\"\"\n",
    "    now = datetime.now().isoformat()\n",
    "    habit = HABIT_STORE[habit_name]\n",
    "    \n",
    "    # Simple streak logic relying on HABIT_STORE state\n",
    "    last_date_str = habit['last_logged'].split('T')[0] if habit['last_logged'] else None\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    if last_date_str != today:\n",
    "        yesterday = (datetime.now() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        if last_date_str != yesterday:\n",
    "             habit['streak'] = 0\n",
    "\n",
    "    habit['streak'] += 1\n",
    "    habit['history'].append({\"timestamp\": now, \"value\": value})\n",
    "    habit['last_logged'] = now\n",
    "    \n",
    "    return habit['streak']\n",
    "\n",
    "def get_adaptive_context(intent: str, habit: Optional[str] = None) -> str:\n",
    "    \"\"\"CUSTOM TOOL: Retrieves Memory/Context for the LLM response.\"\"\"\n",
    "    context = \"\"\n",
    "    # Use USER_PROFILE state for Adaptive Personalization\n",
    "    if USER_PROFILE.get(\"last_failure\") and USER_PROFILE.get(\"last_failure_date\"):\n",
    "        last_fail = USER_PROFILE[\"last_failure\"]\n",
    "        \n",
    "        if intent == \"update_habit\" and habit == last_fail:\n",
    "            context += f\"You successfully logged {habit}, overcoming your last skip on {USER_PROFILE['last_failure_date']}.\"\n",
    "        elif intent == \"request_small_action\":\n",
    "            context += f\"User is low energy about {habit}. Offer simple encouragement.\"\n",
    "            \n",
    "    return context\n",
    "\n",
    "def get_habit_dashboard() -> pd.DataFrame:\n",
    "    \"\"\"Generates the UNSTYLED DataFrame for the dashboard updates.\"\"\"\n",
    "    display_data = []\n",
    "    for name, data in HABIT_STORE.items():\n",
    "        last_log_str = data['last_logged'].split('T')[0] if data['last_logged'] else \"Never\"\n",
    "        display_data.append({\n",
    "            \"Habit\": name.capitalize(),\n",
    "            \"Goal\": f\"{data['goal']} {data['unit']}\",\n",
    "            \"Min Action\": f\"{data['min_version']} {data['unit']}\",\n",
    "            \"Current Streak üî•\": data['streak'],\n",
    "            \"Last Logged\": last_log_str\n",
    "        })\n",
    "    df = pd.DataFrame(display_data)\n",
    "    # Return the raw DataFrame for smooth Gradio updates\n",
    "    return df\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 3. AGENT ORCHESTRATION (CONCEPT: LLM AGENT & SEQUENTIAL LOGIC)\n",
    "# =========================================================================\n",
    "\n",
    "def orchestrate_agent(user_input: str) -> str:\n",
    "    \"\"\"The main agent orchestration loop (Sequential Agent).\"\"\"\n",
    "    \n",
    "    # 1. LLM CLASSIFICATION (Agent Task)\n",
    "    classification = llm_client.get_structured_response(user_input)\n",
    "    print(f\"<- LLM Classification: {classification}\")\n",
    "    \n",
    "    # 2. TOOL/LOGIC EXECUTION (Business Logic)\n",
    "    habit_name = classification.get(\"habit\")\n",
    "    \n",
    "    if classification[\"intent\"] == \"update_habit\" and habit_name in HABIT_STORE:\n",
    "        # CALLS CUSTOM TOOL: update_streak_and_history\n",
    "        value = classification.get(\"value\", 0.0) # Ensure value is retrieved safely\n",
    "        update_streak_and_history(habit_name, value)\n",
    "        \n",
    "    elif classification[\"intent\"] == \"request_small_action\":\n",
    "        # This intent is handled conceptually by the 2-Minute Rule prompt logic\n",
    "        pass \n",
    "        \n",
    "    # 3. GET CONTEXT (CALLS CUSTOM TOOL: get_adaptive_context)\n",
    "    context = get_adaptive_context(classification[\"intent\"], habit_name)\n",
    "\n",
    "    # 4. LLM NLG RESPONSE (Agent Task)\n",
    "    llm_response = llm_client.generate_response(classification, context)\n",
    "    \n",
    "    return llm_response\n",
    "\n",
    "# --- UI Helper Function with Dictionary Format ---\n",
    "def chat_and_update(user_message: str, history: List) -> tuple:\n",
    "    \"\"\"Handles the UI chat interaction and update chain using the modern 'messages' format.\"\"\"\n",
    "    if not user_message:\n",
    "        return history, get_habit_dashboard()\n",
    "    \n",
    "    agent_response = orchestrate_agent(user_message)\n",
    "    \n",
    "    # CRITICAL FIX: Append the new messages as Dictionaries\n",
    "    history.append({\"role\": \"user\", \"content\": user_message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": agent_response})\n",
    "    \n",
    "    updated_dashboard = get_habit_dashboard()\n",
    "    \n",
    "    return history, updated_dashboard\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# 4. GRADIO UI DEMONSTRATION\n",
    "# =========================================================================\n",
    "\n",
    "with gr.Blocks(title=\"TrackIt.AI - LLM Habit Agent\") as demo:\n",
    "    gr.Markdown(\"# TrackIt.AI: The Zero-Friction Agent\")\n",
    "    \n",
    "\n",
    "    # --- Row 1: Dashboard (Visualizing Session State) ---\n",
    "    with gr.Row():\n",
    "        dashboard = gr.DataFrame(\n",
    "            value=get_habit_dashboard, \n",
    "            label=\"Habit Dashboard (Reflects Session State Changes)\", \n",
    "            interactive=False \n",
    "        )\n",
    "\n",
    "    # --- Row 2: Chat & Agent Output ---\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        \n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Agent Conversation\",\n",
    "            height=300,\n",
    "            # CRITICAL FIX: Removed the unsupported 'type' argument.\n",
    "            # History logic in chat_and_update now uses the default 'messages' dictionary format.\n",
    "        )\n",
    "\n",
    "    # --- Row 3: Command Center ---\n",
    "    with gr.Row(variant=\"compact\"):\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Type your habit update here (e.g., 'I completed 9k steps') or use the 'Minimal Win' button...\", \n",
    "            container=False, \n",
    "            scale=4\n",
    "        )\n",
    "        \n",
    "        lazy_button = gr.Button(\"üí° I'm Feeling Lazy (Minimal Win)\", scale=1, variant=\"secondary\")\n",
    "\n",
    "\n",
    "    # --- Event Handling ---\n",
    "    \n",
    "    # 1. Main Submit Handler\n",
    "    msg_submit = msg.submit(\n",
    "        fn=chat_and_update,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, dashboard]\n",
    "    )\n",
    "    \n",
    "    # 2. Lazy Button Handler\n",
    "    lazy_button.click(\n",
    "        fn=lambda: f\"I'm feeling lazy about {USER_PROFILE['last_failure']}.\", \n",
    "        inputs=None, \n",
    "        outputs=msg\n",
    "    ).then(\n",
    "        fn=chat_and_update,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, dashboard]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"### Examples to Try:\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"I completed my 8k steps today.\"], \n",
    "            [\"I did 12 minutes of meditation.\"], \n",
    "            [f\"I'm feeling lazy about {USER_PROFILE['last_failure']}.\"],\n",
    "            [\"I didn‚Äôt finish anything today.\"], \n",
    "        ],\n",
    "        inputs=msg\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "demo.launch(inbrowser=True, share=True, theme=gr.themes.Soft())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec70f92-9c2d-43c5-8744-45d12476a907",
   "metadata": {},
   "source": [
    "---\n",
    "# Explanation of Code\n",
    "Below is detailed explanation of each component of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d294629e-a540-4de5-a426-b9d527fcb95b",
   "metadata": {},
   "source": [
    "### 1. SETUP AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ccbbf-bb37-4665-94b8-f884a31a9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install groq pandas gradio numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc76fec-6677-4c39-b389-26d01f43d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any, List, Optional\n",
    "import gradio as gr\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc127c3-4dcc-43f9-ba83-ec0438df9f6f",
   "metadata": {},
   "source": [
    "### Code Explanation: Setup and Session State (Single-Line Overview)\n",
    "\n",
    "##### LLM Connection\n",
    " `GROQ_MODEL`: Specifies the **high-speed Groq LLM** used for all agent reasoning (classification and response generation).\n",
    "\n",
    "##### Session State (Agent Memory - Sessions & State Management)\n",
    "\n",
    "- `HABIT_STORE`: The agent's **in-memory database** storing current streaks, goals, and history for all habits.\n",
    "- `USER_PROFILE`: Stores essential **personal context** (e.g., last failure) used for adaptive, empathetic coaching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78948270-7ed4-4666-92eb-35eb4491d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Configuration ---\n",
    "# IMPORTANT: Set your Groq API Key (get this from the GroqCloud Console)\n",
    "os.environ[\"GROQ_API_KEY\"] = \"API_KEY_HERE\"\n",
    "# You must have your Groq API Key set as an environment variable or passed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80ffbb2d-48ec-4913-a70c-24ac966b6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Groq Configuration ---\n",
    "# Selecting a fast model. Llama-3 8B or 70B on Groq.\n",
    "GROQ_MODEL = \"llama-3.1-8b-instant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45558973-311c-489e-9130-5a586bafb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global State (Remains the same) ---\n",
    "HABIT_STORE: Dict[str, Any] = {\n",
    "    # ... (rest of your habit store data)\n",
    "}\n",
    "USER_PROFILE = {\"last_failure\": \"meditate\", \"last_failure_date\": \"2025-12-01\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43adc5-c1f3-49d5-a7a0-9a1c960a9c59",
   "metadata": {},
   "source": [
    "### 2. LLM CLIENT AND PROMPT TEMPLATES \n",
    "\n",
    "This class encapsulates the **LLM-Powered Agent** logic, executing the critical two-step reasoning process required by our architecture.\n",
    "\n",
    "1.  **Classification (`get_structured_response`):**\n",
    "   - This is the agent's \"parser\" tool.\n",
    "   - It uses a **rigid system prompt** and strict JSON output formatting ($T=0.0$) to reliably translate messy user input (e.g., \"I only did 5 minutes of reading\") into a clean, structured command (Intent, Habit, Value).\n",
    "   - This is the key to achieving zero-friction input.\n",
    "2.  **Natural Language Generation (NLG) (`generate_response`):**\n",
    "- This is the agent's \"coach.\" It retrieves the latest **Session State** (e.g., streak) and injects adaptive **Context** (e.g., the 2-Minute Rule) into a second prompt.\n",
    "- This allows the LLM to generate a personalized, empathetic, and motivating response, completing the conversational loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d4a1f2-6b19-485c-9c4e-e65b46772d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Groq LLM Client ---\n",
    "class GroqLLMClient:\n",
    "    def __init__(self, model: str):\n",
    "        self.client = Groq()\n",
    "        self.model = model\n",
    "        \n",
    "        # System prompt for structured, constrained output\n",
    "        self.system_prompt = (\n",
    "            \"You are an AI habit agent. Your task is to act as a highly precise JSON parser. \"\n",
    "            \"Based ONLY on the user's input, classify the intent and extract the required data. \"\n",
    "            \"The output MUST be a single JSON object. Do not add any extra text or conversation. \"\n",
    "            \"Possible habits: walk (steps), meditate (minutes), read (minutes). \"\n",
    "            \"Possible intents: update_habit, request_small_action, no_action, unknown.\"\n",
    "            \"If the user asks to log a habit, use 'update_habit'. If the user expresses fatigue or laziness, use 'request_small_action'. \"\n",
    "            \"For 'update_habit', extract 'habit', 'value' (number), and 'unit'. For 'request_small_action', extract 'habit' and set 'value' to null.\"\n",
    "        )\n",
    "\n",
    "    def get_structured_response(self, user_input: str) -> Dict[str, Any]:\n",
    "        \"\"\"Calls Groq to get structured JSON output.\"\"\"\n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"User says: '{user_input}'\"},\n",
    "                ],\n",
    "                model=self.model,\n",
    "                response_format={\"type\": \"json_object\"}, \n",
    "                temperature=0.0 # Use low temperature for reliable classification\n",
    "            )\n",
    "            \n",
    "            # The result is a JSON string in the response content\n",
    "            json_str = chat_completion.choices[0].message.content.strip()\n",
    "            return json.loads(json_str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Groq Classification Error: {e}\")\n",
    "            # Fallback for error handling (demonstrates robustness)\n",
    "            return {\"intent\": \"unknown\", \"error\": str(e)}\n",
    "\n",
    "    def generate_response(self, classification_result: Dict[str, Any], context: str) -> str:\n",
    "        \"\"\"Calls Groq for a personalized, conversational response (NLG).\"\"\"\n",
    "        intent = classification_result.get(\"intent\", \"unknown\")\n",
    "        habit = classification_result.get(\"habit\")\n",
    "        \n",
    "        nlg_prompt = \"\"\n",
    "        if intent == \"update_habit\":\n",
    "            value = classification_result.get(\"value\")\n",
    "            # Adaptive Personalization context is injected here\n",
    "            nlg_prompt = (\n",
    "                f\"The user successfully logged {value} {habit}. \"\n",
    "                f\"Their current streak for {habit} is {HABIT_STORE.get(habit, {}).get('streak', 0)}. \"\n",
    "                f\"The system context is: {context}. \"\n",
    "                \"Generate a short, highly motivating, and personalized response. Encourage them to keep the streak alive.\"\n",
    "            )\n",
    "        elif intent == \"request_small_action\" and habit:\n",
    "            min_val = HABIT_STORE.get(habit, {}).get(\"min_version\", 1)\n",
    "            unit = HABIT_STORE.get(habit, {}).get(\"unit\", \"unit\")\n",
    "            # The 2-Minute Rule is injected here\n",
    "            nlg_prompt = (\n",
    "                f\"The user feels lazy about '{habit}'. \"\n",
    "                f\"Remind them of the '2-Minute Rule': Just ask them to do the bare minimum: '{min_val} {unit}' of {habit}. \"\n",
    "                \"Generate a super low-pressure, encouraging, and guilt-free response.\"\n",
    "            )\n",
    "        else:\n",
    "            nlg_prompt = \"The user input was confusing or logged nothing. Generate a polite, non-guilting message asking them to rephrase their update.\"\n",
    "        \n",
    "        try:\n",
    "            chat_completion = self.client.chat.completions.create(\n",
    "                messages=[{\"role\": \"user\", \"content\": nlg_prompt}],\n",
    "                model=self.model,\n",
    "                temperature=0.7 # Higher temperature for creative, conversational response\n",
    "            )\n",
    "            return chat_completion.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"AI Response Error: Failed to generate response ({e}).\"\n",
    "\n",
    "\n",
    "llm_client = GroqLLMClient(model=GROQ_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4bc68-a565-472b-8c7c-a65747e37fbd",
   "metadata": {},
   "source": [
    "### 3. ORCHESTRATOR LOGIC\n",
    "This code is the operational heart of the system, executing the **LLM-Tool-State loop** and integrating with the Gradio UI.\n",
    "\n",
    "##### 1. `orchestrate_agent` (The Sequential Agent)\n",
    "\n",
    "This function defines the fixed, four-step path for every user input, fulfilling the **LLM-Powered Agent** requirement:\n",
    "\n",
    "1.  **LLM CLASSIFICATION:** Calls the LLM to convert messy user text into a structured JSON command (Intent, Habit, Value).\n",
    "2.  **TOOL EXECUTION:** If the intent is `\"update_habit\"`, it calls the **Custom Tool** `update_streak_and_history`. This securely writes the new progress to the **Session State** (`HABIT_STORE`), updating the streak count.\n",
    "3.  **GET CONTEXT:** Calls the **Custom Tool** `get_adaptive_context` to retrieve personalized context for the LLM.\n",
    "4.  **LLM NLG RESPONSE:** Uses the classified intent and context to generate the final, motivational response.\n",
    "\n",
    "##### 2. `chat_and_update` (UI Bridge)\n",
    "\n",
    "This is the essential helper function that calls `orchestrate_agent`. It then appends both the user and agent messages to the chat history using the required **modern dictionary format** and calls `get_habit_dashboard()` to refresh the dashboard. This visually confirms the success of the agent's state-altering tool execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79d55d3-340f-458c-b40e-0693ee905e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_streak_and_history(habit_name: str, value: float):\n",
    "    \"\"\"Updates the global state based on a successful action.\"\"\"\n",
    "    now = datetime.now().isoformat()\n",
    "    habit = HABIT_STORE[habit_name]\n",
    "    \n",
    "    # Simple streak logic: only reset if the last log was yesterday or earlier\n",
    "    last_date = datetime.fromisoformat(habit['last_logged'].split('T')[0]) if habit['last_logged'] else None\n",
    "    today = datetime.now().split(' ')[0]\n",
    "    \n",
    "    if last_date and last_date.strftime('%Y-%m-%d') != today:\n",
    "        yesterday = (datetime.now() - pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        if last_date.strftime('%Y-%m-%d') != yesterday:\n",
    "             habit['streak'] = 0\n",
    "\n",
    "    habit['streak'] += 1\n",
    "    habit['history'].append({\"timestamp\": now, \"value\": value})\n",
    "    habit['last_logged'] = now\n",
    "    \n",
    "    return habit['streak']\n",
    "\n",
    "def get_adaptive_context(intent: str, habit: Optional[str] = None) -> str:\n",
    "    \"\"\"Retrieves memory for personalized LLM response (Adaptive Personalization).\"\"\"\n",
    "    \n",
    "    context = \"\"\n",
    "    # Retrieve user's last known failure to encourage overcoming it\n",
    "    if USER_PROFILE.get(\"last_failure\") and USER_PROFILE.get(\"last_failure_date\"):\n",
    "        last_fail = USER_PROFILE[\"last_failure\"]\n",
    "        \n",
    "        if intent == \"update_habit\" and habit == last_fail:\n",
    "            context += f\"(Note: This is great, you broke the cycle from your last skip on {USER_PROFILE['last_failure_date']}).\"\n",
    "        elif intent == \"request_small_action\":\n",
    "            context += \"(Note: User is low energy, reference the 'just start' principle.)\"\n",
    "            \n",
    "    return context\n",
    "\n",
    "def orchestrate_agent(user_input: str) -> str:\n",
    "    \"\"\"The main agent function.\"\"\"\n",
    "    print(f\"-> Processing: '{user_input}'\")\n",
    "\n",
    "    # 1. Intent Classification\n",
    "    classification = llm_client.get_structured_response(user_input)\n",
    "    print(f\"<- LLM Classification Result (Structured Data): {classification}\")\n",
    "    \n",
    "    # 2. Business Logic Execution\n",
    "    habit_name = classification.get(\"habit\")\n",
    "    \n",
    "    if classification[\"intent\"] == \"update_habit\":\n",
    "        value = classification[\"value\"]\n",
    "        update_streak_and_history(habit_name, value)\n",
    "        context = get_adaptive_context(classification[\"intent\"], habit_name)\n",
    "    \n",
    "    elif classification[\"intent\"] == \"request_small_action\":\n",
    "        # The habit for the small action is extracted by the mock client\n",
    "        context = get_adaptive_context(classification[\"intent\"], habit_name)\n",
    "\n",
    "    elif classification[\"intent\"] == \"no_action\":\n",
    "        # Future Option: AI could check history here and suggest a small action automatically\n",
    "        context = \"No new action logged.\"\n",
    "        \n",
    "    else: # Unknown intent / fallback\n",
    "        return llm_client.generate_response(classification, \"\")\n",
    "\n",
    "    # 3. AI-Generated Response (NLG)\n",
    "    llm_response = llm_client.generate_response(classification, context)\n",
    "    \n",
    "    return llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04464515-55df-41a9-aea5-6dbd9b9e6169",
   "metadata": {},
   "source": [
    "### 4. INTERACTIVE DEMO AND VISUALIZATION (Gradio Blocks UI) \n",
    "This code sets up the **Gradio interface** and its interactive flow. It uses `gr.Blocks` for structure.\n",
    "\n",
    "* **Row 1 (Dashboard):** Displays `gr.DataFrame` which visualizes the **Session State** (`HABIT_STORE`) via `value=get_habit_dashboard`.\n",
    "* **Row 2 (Chat):** Contains the `gr.Chatbot` for conversation.\n",
    "* **Row 3 (Input):** Holds the `gr.Textbox` (`msg`) and the **Lazy Button**.\n",
    "* **Event Handling:** The `msg.submit` and `lazy_button.click().then()` events both call the core function, `chat_and_update`, passing the chat history and input, and updating the `chatbot` and `dashboard`. This links the UI to the agent's logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb23055-784f-4fa4-99d7-962c08ed6973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://588d1d446dd8c3878d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://588d1d446dd8c3878d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Processing: 'I completed my 8k steps today.'\n",
      "<- LLM Classification Result (Structured Data): {'intent': 'update_habit', 'habit': 'walk', 'value': 8000, 'unit': 'steps'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2134, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Local\\Temp\\ipykernel_22584\\1953051281.py\", line 21, in chat_and_update\n",
      "    agent_response = orchestrate_agent(user_message)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Local\\Temp\\ipykernel_22584\\3343481007.py\", line 49, in orchestrate_agent\n",
      "    update_streak_and_history(habit_name, value)\n",
      "  File \"C:\\Users\\yashv\\AppData\\Local\\Temp\\ipykernel_22584\\3343481007.py\", line 4, in update_streak_and_history\n",
      "    habit = HABIT_STORE[habit_name]\n",
      "            ~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'walk'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Processing: 'I'm feeling lazy about my biggest failure habit.'\n",
      "<- LLM Classification Result (Structured Data): {'intent': 'request_small_action', 'habit': 'biggest failure'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\blocks.py\", line 2117, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\blocks.py\", line 1894, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\components\\chatbot.py\", line 690, in postprocess\n",
      "    self._check_format(value)\n",
      "  File \"C:\\Users\\yashv\\AppData\\Roaming\\Python\\Python312\\site-packages\\gradio\\components\\chatbot.py\", line 400, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: \"Data incompatible with messages format. Each message should be a dictionary with 'role' and 'content' keys or a ChatMessage object.\"\n"
     ]
    }
   ],
   "source": [
    "def get_habit_dashboard() -> pd.DataFrame:\n",
    "    \"\"\"Generates the styled DataFrame for the dashboard.\"\"\"\n",
    "    # ... (function body remains the same)\n",
    "    display_data = []\n",
    "    for name, data in HABIT_STORE.items():\n",
    "        last_log_str = data['last_logged'].split('T')[0] if data['last_logged'] else \"Never\"\n",
    "        display_data.append({\n",
    "            \"Habit\": name.capitalize(),\n",
    "            \"Goal\": f\"{data['goal']} {data['unit']}\",\n",
    "            \"Min Action\": f\"{data['min_version']} {data['unit']}\",\n",
    "            \"Current Streak üî•\": data['streak'],\n",
    "            \"Last Logged\": last_log_str\n",
    "        })\n",
    "    df = pd.DataFrame(display_data)\n",
    "    styled_df = df.style.set_properties(**{'font-weight': 'bold'}, subset=['Current Streak üî•'])\n",
    "    return styled_df.data # Return the Styler object for Gradio\n",
    "\n",
    "def chat_and_update(user_message: str, history: List) -> tuple:\n",
    "    \"\"\"The main function handling chat, agent, and dashboard updates.\"\"\"\n",
    "    # ... (function body remains the same)\n",
    "    if not user_message:\n",
    "        return history, get_habit_dashboard()\n",
    "    agent_response = orchestrate_agent(user_message)\n",
    "    history.append((user_message, agent_response))\n",
    "    updated_dashboard = get_habit_dashboard()\n",
    "    return history, updated_dashboard\n",
    "\n",
    "# --- Gradio UI Block Definition ---\n",
    "# FIX 1 is already applied: theme is in .launch()\n",
    "with gr.Blocks(title=\"TrackIt.AI - LLM Habit Agent\") as demo:\n",
    "    gr.Markdown(\"# üó£Ô∏è TrackIt.AI: The Zero-Friction Agent\")\n",
    "    gr.Markdown(\"Speak or type your updates. The Groq-powered agent classifies your intent and manages your streaks.\")\n",
    "\n",
    "    # --- Row 1: Dashboard ---\n",
    "    with gr.Row():\n",
    "        dashboard = gr.DataFrame(\n",
    "            value=get_habit_dashboard, \n",
    "            label=\"Habit Dashboard (Streaks & Goals)\", \n",
    "            # FIX 2 & 3: Removed 'height', 'min_height', and the redundant 'type=\"pandas\"'\n",
    "            # We rely on the gr.Row() container to manage the size.\n",
    "            interactive=False \n",
    "        )\n",
    "\n",
    "    # --- Row 2: Chat & Input Area ---\n",
    "    with gr.Row(variant=\"panel\"):\n",
    "        \n",
    "        # NOTE: Chatbot height is usually still supported\n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Agent Conversation\",\n",
    "            height=300,\n",
    "            show_copy_button=False\n",
    "        )\n",
    "\n",
    "    # --- Row 3: Command Center (Input) ---\n",
    "    with gr.Row(variant=\"compact\"):\n",
    "        \n",
    "        msg = gr.Textbox(\n",
    "            placeholder=\"Type your habit update here, or use the mic in the chatbox above...\", \n",
    "            container=False, \n",
    "            scale=4\n",
    "        )\n",
    "        \n",
    "        lazy_button = gr.Button(\"üí° I'm Feeling Lazy (Minimal Win)\", scale=1, variant=\"secondary\")\n",
    "\n",
    "\n",
    "    # --- Event Handling ---\n",
    "    \n",
    "    msg_submit = msg.submit(\n",
    "        fn=chat_and_update,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, dashboard]\n",
    "    )\n",
    "    \n",
    "    lazy_button.click(\n",
    "        fn=lambda: \"I'm feeling lazy about my biggest failure habit.\", \n",
    "        inputs=None, \n",
    "        outputs=msg\n",
    "    ).then(\n",
    "        fn=chat_and_update,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, dashboard]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"### Examples to Try:\")\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"I completed my 8k steps today.\"],\n",
    "            [\"I only did 5 minutes of meditation.\"],\n",
    "            [\"I didn‚Äôt finish anything today.\"],\n",
    "            [\"I'm feeling lazy about my reading habit.\"],\n",
    "        ],\n",
    "        inputs=msg\n",
    "    )\n",
    "\n",
    "# Launch the demo\n",
    "# FIX 4 is already applied: theme is in .launch()\n",
    "demo.launch(inbrowser=True, share=True, theme=gr.themes.Soft())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527dc980-4b13-483b-b8cc-308690a4bb7c",
   "metadata": {},
   "source": [
    "### Future Works and Enhancements\n",
    "\n",
    "* **Passive Data Integration (Smartwatch/Fitness App APIs):** \n",
    "    * **Goal:** Eliminate manual logging for passive activities (steps, sleep).\n",
    "    * **Action:** Integrate a new **Custom Tool** to securely pull structured data from external fitness APIs (e.g., Google Fit, Fitbit).\n",
    "* **Voice Input with Custom Wake Word:** \n",
    "    * **Goal:** Achieve true, **zero-friction, ambient interaction**.\n",
    "    * **Action:** Implement **ASR** and **VAD** with a custom wake word (\"Hey TrackIt...\").\n",
    "* **Agent Evaluation and Observability:** \n",
    "    * **Goal:** Ensure system reliability, accuracy, and cost-efficiency.\n",
    "    * **Action:** Implement comprehensive **Observability** to log **Agent Latency** and **Classification Accuracy** for data-driven tuning.\n",
    "* **Multi-Agent System for Modularity:** \n",
    "    * **Goal:** Increase system scalability and stability.\n",
    "    * **Action:** Restructure the architecture into a **Sequential Multi-Agent System** (Classifier Agent $\\rightarrow$ Executor Agent).\n",
    "* **Context Compaction and Long-Term Memory (LTM):** \n",
    "    * **Goal:** Enhance the agent‚Äôs empathetic depth.\n",
    "    * **Action:** Implement a **Memory Bank** that uses **Context Compaction** on older conversations for highly personalized coaching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04319738-cec6-4d37-a41b-14aab605ffb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "###  Project Conclusion\n",
    "\n",
    "The **TrackIt.AI Agent** successfully addresses the critical **Habit Friction Gap** by implementing an intelligent, zero-friction solution.\n",
    "\n",
    "By building a robust **LLM-Tool-State loop**, we transformed ambiguous natural language input into reliable, state-altering transactions. The project demonstrates mastery of the core requirements: using an **LLM-Powered Agent** (Groq) for dynamic decision-making, utilizing **Custom Tools** for secure business logic execution, and leveraging **Sessions & State Management** (`HABIT_STORE`) to provide personalized, context-aware coaching.\n",
    "\n",
    "The resulting system is not merely a tracker; it's an **adaptive, empathetic coach** that minimizes user effort, maximizes data accuracy, and fosters sustained behavioral change through intelligent feedback. Our future work aims to eliminate friction entirely through ambient data and voice integration.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb6d89-ce17-4cef-9215-6a40e947fba3",
   "metadata": {},
   "source": [
    "\n",
    "# Thank You\n",
    "\n",
    "Thank you for the opportunity to demonstrate this project as well as learn and explore the capabilities of LLM Agents, Custom Tools, and State Management. I look forward to your feedback!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
